
File counts after splitting:
  train/raw: 17144
  train/cal: 17144
  test/raw:  4230
  test/cal:  4230

All train/cal files have a matching file in train/raw.

All test/cal files have a matching file in test/raw.

Train unique imageIDs: 118
Test unique imageIDs: 30


//// TRY


!! NORMALIZATION
Data normalization: You're normalizing the FITS data to [0, 1] which is good. Consider:
Tracking the min/max distribution across training set and checking if z-score normalization would work better.
Normalizing calibrated and raw images the same way is important (which you do — great).

!! 5. Transforms
You’re not applying any augmentations. For denoising tasks, you might try:

Horizontal/vertical flip (to encourage generalization).

Brightness variation (if applicable).

Add Gaussian noise to inputs during training to simulate sensor variance.



LOSS - fixed
You're using MSELoss, which works, but might over-smooth the output (since it penalizes large deviations equally across all pixels).

If your target is cleaned images (not segmentation masks), and especially if:

Artifacts are sparse and high-intensity (like solar flares)

You care about preserving fine details (e.g., stars)
Try experimenting with L1 Loss or a combination:


!! 2. Final Output Activation
Your U-Net has no activation at the output. That’s fine for raw regression, but:

Your inputs and outputs are in [0, 1], so consider adding nn.Sigmoid() to self.final_conv1, or clamp the output after forward pass.

Alternatively, leave the model output as-is but clamp it before loss:

outputs = torch.clamp(outputs, 0.0, 1.0)


3. Output Channels
You're using out_channels=3 in the model, which is fine since your FITS images were triplicated to 3 channels.

✅ This makes sense if you're enforcing pixel-wise restoration in all 3 channels.

But if it's truly grayscale data: you could simplify to 1 channel to reduce parameters, unless downstream use expects 3 channels.

4. Saving Predictions - FIXED
In save_predictions, you're using argmax, which only makes sense if the output is multi-class logits.

Since you're using regression and MSELoss, this is likely a bug:

outputs = outputs.clamp(0.0, 1.0)  # Clamp or sigmoid
Then save the raw float tensor, no argmax.

-----

TODO
save best checkpoint
z-score normalization based on whole dataset
Transforms like flips etc


